---
title: "primary1"
author: "Austin Funcheon and Viraj Rane"
date: "5/6/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Preparation

```{r}
# Importing all libraries

library(ggplot2)
library(tidyr)
library(dplyr)
library(randomForest)
library(caret)
library(lattice)
library(lubridate)
library(smotefamily)
library(ROSE)
library(tree)
library(snow)
library(doParallel)
library(vtable)
library(unbalanced)
library(pROC)
library(ROCR)
library(naivebayes)
library(e1071)
library(neuralnet)
library(mlbench)
cl <- makePSOCKcluster(detectCores() -1)
registerDoParallel(cl)
set.seed(123)


```


```{r}
df <- read.csv("train.csv")
dfOriginal <- df
df$Id <- NULL
factors <- c("IsUseful")
df[factors] <- lapply(df[factors], factor)
#sumtable(df)
#str(df)
head(df)
```

```{r}
#sumtable(df, out="csv", file='data summary2', group = 'IsUseful')
```

```{r}
sum(is.na(df))
#no NAs
#table(df$IsUseful)

```
IsUseful
   0    1 
 452 3548
Data is highly unbalanced.

```{r}
#Austin
df0 <- df
train <- sample(1:nrow(df), nrow(df)*0.8)
train_data <- df[train,]
test_data <- df[-train,]

outcome <- table(train_data$IsUseful)
#outcome identify count of !ISUseful as minority class
train_data0 <- train_data
#train_data0

minCount <- outcome[names(outcome)==0]
#minCount
dfBal <- ovun.sample(IsUseful~., data = train_data, method = "under", N = minCount*2)$data

df0Bal <- dfBal
train_data <- dfBal

table(train_data$IsUseful)

```
```{r}
#Austin
start_t <- Sys.time()
cat("",cat(" Variable selection Training started at:",format(start_t, "%a %b %d %X %Y")))

#run a prelim rf for variable summary. 
rf_eval <- randomForest(IsUseful~., data = train_data, ntree=2000,
                           mtry = 5, importance = TRUE)

#rf_eval
varImpPlot(rf_eval) #plot relevance plot

finish_t <- Sys.time()
cat("",cat("Variable selection Training finished at:",format(finish_t, "%a %b %d %X %Y")))

cat("Variable selection The training process finished in",difftime(finish_t,start_t,units="mins"), "minutes")

```
```{r} 
#Austin Identify the lowest impact variables from the RF run
rfImp <- importance(rf_eval)
#rfImp

rf_Imp_Sort <- as.data.frame.matrix(rfImp)
#adds a column for accuracy*gini for overall variable impact
#print(rf_Imp_Sort$MeanDecreaseAccuracy)
#print(rf_Imp_Sort$MeanDecreaseGini)
#rf_Imp_Sort$cross <- rf_Imp_Sort$MeanDecreaseAccuracy
rf_Imp_Sort$cross <- rf_Imp_Sort$MeanDecreaseAccuracy*rf_Imp_Sort$MeanDecreaseGini
#rf_Imp_Sort$cross
#adds an overall rank of impact of variable, with bigger being biggest ranked impact, with 1 being lowest impact
rf_Imp_Sort$rank <- rank(rf_Imp_Sort$cross)

#rf_Imp_Sort

#variables ranked by cross order
rf_Imp_SO <- rf_Imp_Sort[order(-rf_Imp_Sort$cross),]
#rf_Imp_SO
#variable list sorted by importance. Accuracy * Gini
```

```{r} 
#Austin Check accuracy score against test data.
rf_yhat <- predict(rf_eval, newdata = test_data, type="prob")
#accuracy score of mtry 5 rf used for variable selection
rfscore <- postResample(rf_yhat, test_data$IsUseful)
print(rfscore)

#Viraj check this think this is auc with IsUseful=1
result.roc <- roc(test_data$IsUseful, rf_yhat[,1])
plot(result.roc, print.thres="best", print.thres.best.method="closest.topleft", print.auc=TRUE)
rfAUC <- auc(result.roc)
rfAUC

```

```{r}
#Austin 
rfVarDrop <-rf_Imp_Sort
#trim the less than chance predictors
rfVarDrop <- rfVarDrop %>% filter(MeanDecreaseAccuracy <= 0)

dropVar <- row.names(rfVarDrop) #Variables to drop list 
#dropVar
#make a shorter list of variables from rf variable selection
train_trim_data <- train_data[ , ! names(train_data) %in% dropVar] 
test_trim_data <- test_data[ , ! names(test_data) %in% dropVar] 
```


```{r}
#Austin  # 1 minute run time
start_t <- Sys.time()
cat("",cat("Trimmed Training started at:",format(start_t, "%a %b %d %X %Y")))

rf_eval2 <- randomForest(IsUseful~., data = train_trim_data, ntree=2000,
                           mtry = 5, importance = TRUE)

rf_eval2
varImpPlot(rf_eval2)

finish_t <- Sys.time()
cat("",cat("Trimmed Training finished at:",format(finish_t, "%a %b %d %X %Y")))

cat("Trimmed The training process finished in",difftime(finish_t,start_t,units="mins"), "minutes")
```
```{r} 
#Austin Check accuracy score against test data.
rf_yhat2 <- predict(rf_eval2, newdata = test_trim_data, type="prob")
#accuracy score of mtry 5 rf used for variable selection
#rfscore2 <- postResample(rf_yhat, test_data$IsUseful)
#print(rfscore2)

#Viraj check this think this is auc with IsUseful=1
result.roc2 <- roc(test_trim_data$IsUseful, rf_yhat2[,1])
plot(result.roc2, print.thres="best", print.thres.best.method="closest.topleft", print.auc=TRUE)
rfAUC2 <- auc(result.roc2)
rfAUC2
```


```{r}
tuneGrid <- data.frame(mtry = 1:9)
#tuneGrid
```

```{r}
#Austin
control <- trainControl(method = 'repeatedcv', 
                        number = 10,
#                        classProbs = TRUE,
                        repeats = 1)

# print out system time before training
start_t <- Sys.time()
cat("",cat("Training started at:",format(start_t, "%a %b %d %X %Y")))

#rf_tuned <- train(IsUseful~ ., data = train_data,
#                  method = 'rf',
#                  trControl = control,
#                  metric = 'ROC'
#                  tuneGrid = tuneGrid)

# print out system time after training
finish_t <- Sys.time()
cat("",cat("Training finished at:",format(finish_t, "%a %b %d %X %Y")))

cat("The training process finished in",difftime(finish_t,start_t,units="mins"), "minutes")

#print(rf_tuned)
```

```{r} 
#Austin Check accuracy score against test data.
#rf_tune <- predict(rftrim_tuned, newdata = test_trim_data, type="prob")

#result.roc3 <- roc(test_data$IsUseful, rf_tune[,1])
#plot(result.roc3, print.thres="best", print.thres.best.method="closest.topleft", print.auc=TRUE)
#rfAUC3 <- auc(result.roc3)
#rfAUC3
```

### Support Vector Machine


```{r}
#no point in this segment for scaling?
#preprocessParams <- preProcess(train_data, method = c("scale","center"))
#print(preprocessParams)

```



```{r}
#Austin Tune the cost and gamma parameters for radial kernel
start_t <- Sys.time()
cat("",cat(" RBF SVM Training started at:",format(start_t, "%a %b %d %X %Y")))

tune_svm_radial <- tune(svm, IsUseful ~ ., data = train_data,
                       kernel = 'radial',
                       tunecontrol=tune.control(cross=10,sampling="cross"),
  #                     ranges =list(cost=10^(2),gamma=10^(-2)))
                       ranges =list(cost=10^(-2:2),gamma=10^(-2:2)))
finish_t <- Sys.time()
cat("",cat("RBF SVM Training finished at:",format(finish_t, "%a %b %d %X %Y")))

cat("The RBF SVM training process finished in",difftime(finish_t,start_t,units="mins"), "minutes")


# Print the best parameters
tune_svm_radial$best.parameters
```

```{r} 
#Austin Check accuracy score against test data.
svm_yhat <- predict(tune_svm_radial$best.model, newdata = test_data, type="prob")

#this is wrong. coded 2 and 1 instead of 0 and 1 Viraj idiot proof me I can't figure it out.
result.rocSVM <- roc(test_data$IsUseful, as.numeric(as.factor(svm_yhat)))
#table(test_data$IsUseful)
#table(as.numeric(svm_yhat))

plot(result.rocSVM, print.thres="best", print.thres.best.method="closest.topleft", print.auc=TRUE)
rfAUC3 <- auc(result.rocSVM)
rfAUC3
```

```{r}
#Austin Naive Bayes 
nb <- naive_bayes(IsUseful ~ ., data = train_data, usekernel = T) 
nb_Out<- predict(nb, newdata = test_data, type="prob")
nb_OutC <- ifelse(nb_Out > 0.5, 1, 0)
#table(nb_Out[,1])

result.roc4 <- roc(test_data$IsUseful, nb_OutC[,1])
plot(result.roc4, print.thres="best", print.thres.best.method="closest.topleft", print.auc=TRUE)
nbAUC <- auc(result.roc4)
nbAUC
```
```{r}
#Viraj
f <- as.formula(IsUseful ~ .)

# Fit a neural network model with 2 hidden layers
nn_fit_2 <- neuralnet(f, data = train_data, hidden = c(2,1), linear.output=TRUE) 
# Show results
summary(nn_fit_2)

```


```{r, fig.height=6, fig.width=7}
plot(nn_fit_2,rep="best", cex=0.8)
```

```{r}
# Viraj Fit a neural network model with 1 hidden layer
nn_fit_1 <- neuralnet(f, data = train_scaled, hidden = 2) # 20 mins

# Show results
summary(nn_fit_1)
```

```{r,fig.height=6, fig.width=7}
#plotting neural net model with 2 hidden layers
plot(nn_fit_1, rep="best",cex=0.8)
```

# Evaluating Predictive Performance of the Two-Hidden-Layer Model

We use compute() method in the neuralnet package for predicting the performance.

```{r}
#viraj
# Computing the outputs of all neurons for specific arbitrary co variate vectors given a trained neural network
pred2 <- compute(nn_fit_2, test_data[-1])
pred2 <- pred2$net.result
str(pred2)
```

Calculating the prediction performance.
```{r}
#viraj
result.roc5 <- roc(test_data$IsUseful, pred2[,1])
plot(result.roc5, print.thres="best", print.thres.best.method="closest.topleft", print.auc=TRUE)
annAUC <- auc(result.roc5)
annAUC
```

# Evaluating Predictive Performance of the One-Hidden-Layer Model

```{r}
#Viraj
# Computes the outputs of all neurons for specific arbitrary co variate vectors given a trained neural network
pred1 <- compute(nn_fit_1, test_data[-1])
pred1 <- pred1$net.result
```

Calculating the prediction performance.

```{r}
#Viraj
options(scipen=999)
result.roc6 <- roc(test_data$IsUseful, pred1[,1])
plot(result.roc6, print.thres="best", print.thres.best.method="closest.topleft", print.auc=TRUE)
ann1AUC <- auc(result.roc6)
ann1AUC
```

```{r}
#Create Validation Set
val_data <- read.csv("test.csv")
valOriginal <- val_data
val$Id <- NULL
val_data[factors] <- lapply(val_data[factors], factor)
```


```{r} 
#Austin Final model check
#valFinal <- predict(<Insert model here>, newdata = val_data, type="prob")

#Viraj check this think this is auc with IsUseful=1
#result.rocF <- roc(valFinal$IsUseful, <Insert model here>[,1])
#plot(result.rocF, print.thres="best", print.thres.best.method="closest.topleft", print.auc=TRUE)
#AUCF <- auc(result.rocf)
#AUCF
```


```{r}
#stopCluster(cl)
```



