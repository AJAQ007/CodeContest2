---
title: "primary1"
author: "Austin Funcheon and Viraj Rane"
date: "5/6/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Preparation

```{r}
# Importing all libraries
library(caret)
library(ggplot2)
library(tidyr)
library(dplyr)
library(randomForest)
library(lattice)
library(lubridate)
library(smotefamily)
library(ROSE)
library(tree)
library(snow)
library(doParallel)
library(vtable)
library(unbalanced)
library(pROC)
library(ROCR)
library(naivebayes)
library(e1071)
cl <- makePSOCKcluster(detectCores() -1)
registerDoParallel(cl)
set.seed(123)


```


```{r}
df <- read.csv("train.csv")
dfOriginal <- df
df$Id <- NULL
factors <- c("IsUseful")
df[factors] <- lapply(df[factors], factor)
#sumtable(df)
#str(df)
head(df)
```

```{r}
#sumtable(df, out="csv", file='data summary2', group = 'IsUseful')
```

```{r}
sum(is.na(df))
#no NAs
#table(df$IsUseful)

```
IsUseful
   0    1 
 452 3548
Data is highly unbalanced.

```{r}
df0 <- df
train <- sample(1:nrow(df), nrow(df)*0.8)
train_data <- df[train,]
test_data <- df[-train,]

outcome <- table(train_data$IsUseful)
#outcome identify count of !ISUseful as minority class
train_data0 <- train_data
#train_data0

minCount <- outcome[names(outcome)==0]
minCount
dfBal <- ovun.sample(IsUseful~., data = train_data, method = "under", N = minCount*2)$data

df0Bal <- dfBal
train_data <- dfBal

table(train_data$IsUseful)

```
```{r}
start_t <- Sys.time()
cat("",cat(" Variable selection Training started at:",format(start_t, "%a %b %d %X %Y")))

#run a prelim rf for variable summary. 
rf_eval <- randomForest(IsUseful~., data = train_data, ntree=2000,
                           mtry = 5, importance = TRUE)

#rf_eval
varImpPlot(rf_eval) #plot relevance plot

finish_t <- Sys.time()
cat("",cat("Variable selection Training finished at:",format(finish_t, "%a %b %d %X %Y")))

cat("Variable selection The training process finished in",difftime(finish_t,start_t,units="mins"), "minutes")

```
```{r} 
#Austin Identify the lowest impact variables from the RF run
rfImp <- importance(rf_eval)
#rfImp

rf_Imp_Sort <- as.data.frame.matrix(rfImp)
#adds a column for accuracy*gini for overall variable impact
#print(rf_Imp_Sort$MeanDecreaseAccuracy)
#print(rf_Imp_Sort$MeanDecreaseGini)
#rf_Imp_Sort$cross <- rf_Imp_Sort$MeanDecreaseAccuracy
rf_Imp_Sort$cross <- rf_Imp_Sort$MeanDecreaseAccuracy*rf_Imp_Sort$MeanDecreaseGini
#rf_Imp_Sort$cross
#adds an overall rank of impact of variable, with bigger being biggest ranked impact, with 1 being lowest impact
rf_Imp_Sort$rank <- rank(rf_Imp_Sort$cross)

#rf_Imp_Sort

#variables ranked by cross order
rf_Imp_SO <- rf_Imp_Sort[order(-rf_Imp_Sort$cross),]
#rf_Imp_SO
#variable list sorted by importance. Accuracy * Gini
```

```{r} 
#Austin Check accuracy score against test data.
rf_yhat <- predict(rf_eval, newdata = test_data, type="prob")
#accuracy score of mtry 5 rf used for variable selection
rfscore <- postResample(rf_yhat, test_data$IsUseful)
print(rfscore)

#Viraj check this think this is auc with IsUseful=1
result.roc <- roc(test_data$IsUseful, rf_yhat[,1])
plot(result.roc, print.thres="best", print.thres.best.method="closest.topleft", print.auc=TRUE)
rfAUC <- auc(result.roc)
rfAUC

```

```{r}
#Austin 
rfVarDrop <-rf_Imp_Sort
#trim the bottom x predictors
rfVarDrop <- rfVarDrop %>% filter(MeanDecreaseAccuracy <= 0)

dropVar <- row.names(rfVarDrop) #Variables to drop list 
#dropVar
#make a shorter list of variables from rf variable selection
train_trim_data <- train_data[ , ! names(train_data) %in% dropVar] 
test_trim_data <- test_data[ , ! names(test_data) %in% dropVar] 
```


```{r}
#Austin  #30 second run time with 4 cores
start_t <- Sys.time()
cat("",cat("Trimmed Training started at:",format(start_t, "%a %b %d %X %Y")))

rf_eval2 <- randomForest(IsUseful~., data = train_trim_data, ntree=2000,
                           mtry = 5, importance = TRUE)

rf_eval2
varImpPlot(rf_eval2)

finish_t <- Sys.time()
cat("",cat("Trimmed Training finished at:",format(finish_t, "%a %b %d %X %Y")))

cat("Trimmed The training process finished in",difftime(finish_t,start_t,units="mins"), "minutes")
```
```{r} 
#Austin Check accuracy score against test data.
rf_yhat2 <- predict(rf_eval2, newdata = test_trim_data, type="prob")
#accuracy score of mtry 5 rf used for variable selection
#rfscore2 <- postResample(rf_yhat, test_data$IsUseful)
#print(rfscore2)

#Viraj check this think this is auc with IsUseful=1
result.roc2 <- roc(test_trim_data$IsUseful, rf_yhat2[,1])
plot(result.roc2, print.thres="best", print.thres.best.method="closest.topleft", print.auc=TRUE)
rfAUC2 <- auc(result.roc2)
rfAUC2
```


```{r}
#Austin Run Tune on RF  took 8 minute s with 4:15
tuneGrid <- data.frame(mtry = 4:15)



control <- trainControl(method = 'cv', number = 5)
# print out system time before training
#start_t <- Sys.time()
#cat("",cat("RFtune Training started at:",format(start_t, "%a %b %d %X %Y")))

#rftuned <- train(IsUseful ~ ., data = train_data,
#              method = 'rf',
#              trControl = control,
#              tuneGrid = tuneGrid)

# print out system time after training
#finish_t <- Sys.time()
#cat("",cat("RFTune Training finished at:",format(finish_t, "%a %b %d %X %Y")))

#cat(" RFTune The training process finished in",difftime(finish_t,start_t,units="mins"), "minutes")

#print(rftrim_tuned)

```

```{r} 
#Austin Check accuracy score against test data.
#rf_tune <- predict(rftrim_tuned, newdata = test_trim_data, type="prob")

#result.roc3 <- roc(test_data$IsUseful, rf_tune[,1])
#plot(result.roc3, print.thres="best", print.thres.best.method="closest.topleft", print.auc=TRUE)
#rfAUC3 <- auc(result.roc3)
#rfAUC3
```

### Support Vector Machine


```{r}
#preprocessParams <- preProcess(train_data, method = c("scale","center"))
#print(preprocessParams)

```



```{r}
# Tune the cost and gamma parameters for radial kernel
start_t <- Sys.time()
cat("",cat(" RBF SVM Training started at:",format(start_t, "%a %b %d %X %Y")))

tune_svm_radial <- tune(svm, IsUseful ~ ., data = train_data,
                       kernel = 'radial',
                       tunecontrol=tune.control(cross=10,sampling="cross"),
  #                     ranges =list(cost=10^(2),gamma=10^(-2)))
                       ranges =list(cost=10^(-2:2),gamma=10^(-2:2)))
finish_t <- Sys.time()
cat("",cat("RBF SVM Training finished at:",format(finish_t, "%a %b %d %X %Y")))

cat("The RBF SVM training process finished in",difftime(finish_t,start_t,units="mins"), "minutes")


# Print the best parameters
tune_svm_radial$best.parameters
```

```{r} 
#Austin Check accuracy score against test data.
rf_yhat3 <- predict(tune_svm_radial$best.model, newdata = test_data, type="prob")

summary(rf_yhat3)
#print(rf_yhat3)
#print(test_data$IsUseful)
summary(test_data$IsUseful)

#Why is this misdimensioned? grrr.
#result.rocSVM <- roc(test_data$IsUseful, rf_yhat3[,1])
#plot(result.rocSVM, print.thres="best", print.thres.best.method="closest.topleft", print.auc=TRUE)
#rfAUC3 <- auc(result.roc3)
#rfAUC3
```

```{r}
nb <- naive_bayes(IsUseful ~ ., data = train_data, usekernel = T) 
nb_Out<- predict(nb, newdata = test_data, type="prob")

result.roc4 <- roc(test_data$IsUseful, nb_Out[,1])
plot(result.roc4, print.thres="best", print.thres.best.method="closest.topleft", print.auc=TRUE)
rfAUC4 <- auc(result.roc4)
rfAUC4
```


```{r}
#stopCluster(cl)
```



